\newcommand{\nsum}{\sum^n_{i=1}}
\newcommand{\nsumx}{\sum^n_{i=1}x_i}
\newcommand{\nsumy}{\sum^n_{i=1}y_i}
\newcommand{\nsumxx}{\sum^n_{i=1}x_i^2}
\newcommand{\nsumxy}{\sum^n_{i=1}x_iy_i}

\subsection{Proložení dat funkcí}
Ve fyzikálních experimentech obvykle měříme veličiny, které jsou závislé na
veličině jiné, která se během experimentu mění. Vztah mezi závislou a
nezávislou veličinou jsme schopni vysledovat pomocí jejich měření a následného
hledání závislosti.  Nezávislou veličinou velice často bývá čas, ale také jí
může být např.~teplota, síla, poloha a další.~\cite{praktikum}
 
Při hledání oné souvislosti mnohdy potřebujeme proložit naměřená data funkcí.
Proložen dat nám totiž umožňuje ověřit hypotetický předpoklad, zjistit přesnost
měření či může dopomoci k nalezení různých koeficientů a parametrů experimentu.

\subsubsection{Aproximace a interpolace}
Při prokládání bodů funkcí se můžeme přiklonit k jedné ze dvou metod: k
aproximaci, nebo interpolaci.

\paragraph{Interpolace} je proces, kdy se snažíme nalézt takovou funkci, jež
propojí všechny nám známé body. Nejjednodušší metodou, jak toho dosáhnout, je
využití lineární interpolace, kdy každé dvě po sobě následující hodnoty
propojíme přímkou. Můžeme se s ní setkat například při vytváření grafů
v~tabulkových editorech (např.~MS Excel či LibreOffice Calc), kdy propojení
bodů přímkou je obvykle výchozí možnost vykreslování grafů. 

Nevýhoda této metody je ovšem ostrost funkce. Funkce je sice spojitá, ale není
diferencovatelná na celém svém definičním oboru, což omezuje možnosti jejího
následného využití. Kvůli své ostrosti zároveň tato metoda mnohdy nepředstavuje
reálný průběh původní funkce.~\cite{segeth}

Tyto problémy řeší méně rozšířené metody, jakými jsou kupříkladu kvadratická
interpolace či interpolace vyšším polynomem (příkladem může být známý a
rozšířený Lagrangeův polynom). Ty zmenšují interpolační odchylky a díky
definici polynomem jsou diferencovatelné na celé definičním oboru. Ovšem jsou
také složitější na spočítání, jak už z pohledu matematiky, tak z pohledu
výpočetního výkonu.

\paragraph{Aproximace} se od interpolace liší v jednom zásadním aspektu:
nevyžaduje, aby výsledná funkce procházela všemi body. To nám umožňuje najít
o~mnoho hladší funkce, které kopírují průběh dat, či získat neznámé parametry
závislosti z dat s možnou chybu měření. 

Z tohoto důvodu je aproximace vhodnější při prokládání experimentálně získaných
dat funkcí a umožňujeme nám porovnat jednotlivé předpisy funkcí a jejich
korelaci s daty.

\subsubsection{Metoda nejmenších čtverců}
\label{sec:čtverce}
Při našich experimentech měříme velké množství dat a přirozeně chceme všechna z
nich využít v aproximaci naší funkce, čehož právě metoda nejmenších čtverců
dosahuje. Ta je založena na principu minimalizace součtu druhých mocnin%
\footnote{Pojmy \emph{druhá mocnina} a \emph{čtverec} jsou zaměnitelné, protože
plocha čtverce je druhou mocninou jeho strany.} odchylek mezi naměřenými daty
a~aproximovanou funkcí. Příkladem této metody může být
graf~\ref{graph:leastsquares}.

\plotfig{graf/ctverce.tex}[Ukázka metody nejmenších čtverců][graph:leastsquares]

Označme si naší teoretickou funkci jako $f(x)$, ve které figurují kupříkladu
tři neznámé označené jako $a$, $b$ a $c$. Do funkce~$f$ tak vstupuje
proměnná~$x$ a neznámé parametry~$a,b,c$. Aproximace funkce pomocí metody
nejmenších čtverců je tedy potom o nalezení takové ideální hodnoty $a,b,c$, aby
součet čtverců odchylek byl co nejmenší.

Důležitou otázkou na zodpovězení je, proč se vlastně snažíme najít minimum sumy
čtverců odchylek. Pokud máme obecný předpis funkce~$f(x)$, tak víme, že po
dosazení každé jedné naměřené hodnoty~$x_i$ bychom měli dostat výslednou
hodnotu měření~$y_i$. Rozdíl mezi touto teoretickou hodnotu~$f(x_i)$ a ve
skutečnosti naměřenou hodnotou~$y_i$ si označme jako~$\Delta_i = f(x_i)-y_i$.
Při aproximaci bychom tak mohli chtít jednoduše minimalizovat tyto jednotlivé
rozdíly~$\Delta_i$, neboli minimalizovat funkci~$\nsum \Delta_i$. Avšak tu není
možné minimalizovat, protože minimum jakéhokoliv součtu je vždy $-\infty$.
Abychom se tedy vyhnuli tomuto problému, sčítáme hodnoty rozdílů umocněné na
druhou~$\nsum \Delta_i^2$, přičemž minimum této funkce se nachází
v~bodě~$y=0$.~\cite{praktikum}

\plotfig{graf/deltas.tex}[Příklad rozdílů naměřených a teoretických pro~$n=3$]

Obecně tedy můžeme metodu nejmenších čtverců vyjádřit jako hledaní ideálních 
parametrů funkce~$f(x)$ pro minimalizace funkce~$S$, pro kterou platí
\eq{
    S = \nsum(y_i - f(x_i))^2\,.\lbl{eq:general_squares}\\
}
Tento problém se řeší dvěma hlavními metodami: iterativně a analyticky.

\paragraph{Iterativní řešení} je obecné a~fungující pro každý předpis. Problém
nejmenších čtverců řeší pomocí opakování postupu ve smyčce (tzv.~iterování),
kdy se s~každým opakováním vypočítané hodnoty jemně změní. Ověří se, zdali se
funkce~\eqref{eq:general_squares} přiblížila svému minimu a~pokračuje se v~další
změně hodnot. V~moment dosáhnutí minima s~určitou přesností se opakování
zastavuje a~my říkáme, že funkce \emph{konvergovala}.
%Pracuje na principu měnění proměnných, kdy s každou iterací se funkce
%přibližuje správnému výsledku, dokud funkce nekonverguje. 

Tato metoda je používána pro řešení nelineárních problémů, které nejsme schopni
analyticky řešení. Vzhledem k~počtu potřebných výpočtů je aplikována ve formě
počítačových algoritmů, například pomocí algoritmu Levenberg-Marquardtova.

\paragraph{Analytické řešení} funguje na principu nalezení minima pomocí
parciálního derivování podle všech neznámých parametrů. Z těchto derivací
následně vzniká soustava rovnic, kterou jsme schopni vyřešit. Je využívaná pro
řešení lineárních problémů, kdy můžeme naší rovnici zapsat pomocí polynomu
$n$-tého řádu.


\subsubsection{Lineární regrese}
Speciální případ analytického řešení je lineární regrese. Jedná se o případ,
kdy experimentálně získaná data prokládáme lineární funkcí s obecným předpisem
\eq{
    f(x) = ax + b\,.
}
V sekci~\ref{sec:čtverce} jsme si definovali rovnici~\eqref{eq:general_squares},
do které tento obecný lineární předpis můžeme dosadit a tím si vyjádřit rovnici
sumy konkrétně pro lineární funkci:
\eq[m]{
    S = \nsum(y_i-ax_i-b)^2\,.
}
Abychom proložili data funkcí, musíme nalézt minimum funkce~$S$. Toho dosáhneme
pomocí položení jednotlivých parciálních derivací této funkce do rovnosti 
s~nulou.~\cite{wolfram} 

%\begin{minipage}{0.45\textwidth}
%\eq{
%    \pder{S}{a} &= \nsum \(2\(y_i-ax_i-b\)*\(0-x_i-0\)\)\\
%    \pder{S}{a} = -2\nsum x_i\(y_i-ax_i-b\)
%}
%\end{minipage}
%\begin{minipage}{0.55\textwidth}
%\eq{
%    \pder{S}{b} &= \nsum \(2\(y_i-ax_i-b\)*\(0-0-1\)\)\\
%    \pder{S}{b} = -2\nsum \(y_i-ax_i-b\)
%}
%\end{minipage}
%
%\begin{minipage}{0.45\textwidth}
%\eq[m]{
%    -2\nsum x_i\(y_i-ax_i-b\)&=0\\
%    \nsum \(y_ix_i-ax_i^2-bx_i\)&=0\\
%    \nsum y_ix_i -\nsum ax_i^2 -\nsum bx_i&=0\\
%    a\nsumxx +b\nsumx&=\nsumxy
%}
%\end{minipage}
%\begin{minipage}{0.55\textwidth}
%\eq[m]{
%    -2\nsum \(y_i-ax_i-b\) &= 0\\
%    \nsum y_i -\nsum ax_i -\nsum b &= 0\\
%    a\nsumx + nb &= \nsumy
%}
%\end{minipage}

\noindent
\begin{minipage}{0.49\linewidth}
    \begin{gather*}
        \pder{S}{a} = -2\nsum x_i\(y_i-ax_i-b\)\\
        -2\nsum x_i\(y_i-ax_i-b\)=0\\
        a\nsumx + nb = \nsumy \lbl{eq:a_def}
    \end{gather*} 
\end{minipage}
\begin{minipage}{0.50\linewidth}
    \begin{gather*}
        \pder{S}{b} = -2\nsum \(y_i-ax_i-b\)\\
        -2\nsum \(y_i-ax_i-b\) = 0\\
        a\nsumx + nb = \nsumy \lbl{eq:b_def}
    \end{gather*} 
\end{minipage}
\vspace{0cm}

Po jednotlivém derivování funkce~$S$ podle~$a$ a $b$ a upravení výrazů
dostáváme dvě rovnice~\eqref{eq:a_def} a~\eqref{eq:b_def}. Ty nám popisují onu
lineární funkci jíž aproximujeme a $a$ a $b$ zde figurují jako proměnné. Ty
ovšem v tento moment neznáme, tedy pro nás to jsou v tomto případě neznámé.
To znamená, že se jedná o soustavu dvou rovnic o dvou neznámých, kterou můžeme
jednoduše pomocí dosazovací metody vyřešit pro~$a$.
\eq[m]{
    a\nsumx+nb = \nsumy\quad&\Rightarrow\quad 
        b=\frac{\nsumy-a\nsumx}{n}\lbl{eq:b_from_def}\\
    a\nsumxx+b\nsumx=\nsumxy\quad&\Rightarrow\quad 
        a\nsumxx+\frac{\nsumy-a\nsumx}{n}\nsumx=\nsumxy\\
    a&=\frac{n\nsumxy-\nsumx\nsumy}{n\nsumxx-\(\nsumx\)^2}\lbl{eq:a}
}
%\vspace{-6.7ex}
%\begin{gather*}
%    a\nsumxx+\frac{n}\nsumy\nsumx-\frac{n}a\(\nsumx\)^2=\nsumxy\\
%    an\nsumxx-a\(\nsumx\)^2=n\nsumxy-\nsumx\nsumy\\
%\end{gather*}

A protože nyní máme rovnici~\eqref{eq:a} pro $a$, můžeme jí dosadit do dříve
odvozené rovnice~\eqref{eq:b_from_def} pro $b$, čímž dostáváme řešení této
soustavy pro obě neznáme v podobě rovností~\eqref{eq:a} a~\eqref{eq:b}.
\eq[m]{
    b&=\frac{\nsumy-a\nsumx}{n}\\
    b&=\frac{\nsumy-\frac{n\nsumxy-\nsumx\nsumy}{n\nsumxx-\(\nsumx\)^2}\nsumx}{n}\\
%    b&=\frac{n\nsumxx\nsumy-\nsumy\(\nsumx\)^2-n\nsumxy\nsumx+\nsumy\(\nsumx\)^2}
%    {n^2\nsumxx-n\(\nsumx\)^2}\\
    b&=\frac{\nsumxx\nsumy-\nsumxy\nsumx}{n\nsumxx-\(\nsumx\)^2}\lbl{eq:b}
}
